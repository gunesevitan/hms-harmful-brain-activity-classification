dataset:
  eeg_dataset: 'eeg_subsample'
  normalize_targets: True

model:
  model_class: 'EfficientNet'
  model_args:
    model_name: 'efficientnetv2_rw_t'
    pretrained: True
    backbone_args:
      in_chans: 1
      drop_rate: 0.1
      drop_path_rate: 0.1
    pooling_type: 'avg'
    dropout_rate: 0.
    head_args:
      output_dimensions: 6
  model_checkpoint_path: null


training:
  #folds: ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']
  folds: ['fold1', 'fold2', 'fold3', 'fold4',]
  training_sample_qualities: [2]
  input_dimensions: 2
  loss_function: 'BCEWithLogitsLoss'
  loss_function_args: {}
  training_batch_size: 64
  test_batch_size: 128
  num_workers: 16
  random_state: 42
  deterministic_cudnn: False
  device: 'cuda'
  task_type: 'binary'
  optimizer: 'AdamW'
  optimizer_args:
    lr: 0.0005
    betas: [0.9, 0.999]
    weight_decay: 0.0001
  lr_scheduler: 'CosineAnnealingLR'
  lr_scheduler_args:
    T_max: 500
    eta_min: 0.0001
    last_epoch: -1
  amp: True
  epochs: 30
  early_stopping_metric: 'kl_divergence'
  early_stopping_patience: 0

test:
  folds: ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']
  model_file_names: [
    'model_fold_1_epoch_8_best_sq_2_kl_divergence_0.4371.pt',
    'model_fold_2_epoch_19_best_sq_2_kl_divergence_0.3866.pt',
    'model_fold_3_epoch_19_best_sq_2_kl_divergence_0.4017.pt',
    'model_fold_4_epoch_30_best_sq_2_kl_divergence_0.4083.pt',
    'model_fold_5_epoch_19_best_sq_2_kl_divergence_0.4585.pt'
  ]
  tta: True
  tta_flip_dimensions: [[1], [2], [1, 2]]

transforms:
    stationary_period_random_subsample: 1.0
    vertical_flip_probability: 0.5
    horizontal_flip_probability: 0.5
    center_temporal_dropout_time_steps: 200
    center_temporal_dropout_probability: 0.2
    non_center_temporal_dropout_time_steps: 1000
    non_center_temporal_dropout_probability: 0.1

persistence:
  save_best_metrics: ['sq_2_kl_divergence', 'kl_divergence']
  save_epochs: []